{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pneumonia Classifier\n",
    "This notebook we will use CNN to train a classifier that will predict Pneumonia using x-ray's as input.\n",
    "Use the database from: https://www.kaggle.com/paultimothymooney/chest-xray-pneumonia  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain shape:  (5216, 127, 127)\n",
      "Ytrain size:  5216\n",
      "Xtest shape:  (624, 127, 127)\n",
      "Ytest size:  624\n",
      "Xval shape:  (16, 127, 127)\n",
      "Yval size:  16\n",
      "input size:  5856\n",
      "another input size:(should be same)  5856\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "\n",
    "max_size = 127\n",
    "\n",
    "def loadData(path,label):\n",
    "    count = 0\n",
    "    Xresult,Yresult,shapes_result,debug = [],[],[],[]\n",
    "    for filename in os.listdir(path):\n",
    "        if filename.endswith(\".jpeg\"):   \n",
    "            read_img = np.array(Image.open(path+\"/\"+filename).convert(\"L\").resize((max_size,max_size),Image.ANTIALIAS))/255.\n",
    "            Xresult.append(read_img)\n",
    "            Yresult.append(label)\n",
    "            shapes_result.append(read_img.shape)\n",
    "            debug.append(filename)\n",
    "            count += 1\n",
    "    return np.array(Xresult),Yresult,shapes_result,debug\n",
    "\n",
    "shapes,debug = [],[]\n",
    "NORMAL = 0\n",
    "PNEUMONIA = 1\n",
    "\n",
    "Xnormal,Ynormal,shapes_normal,debug_normal = loadData('chest_xray/train/NORMAL',NORMAL)\n",
    "Xpneumonia,Ypneumonia,shapes_pneumonia,debug_pneumonia = loadData('chest_xray/train/PNEUMONIA',PNEUMONIA)\n",
    "Xtrain = np.vstack((Xnormal,Xpneumonia))\n",
    "Ytrain = Ynormal + Ypneumonia\n",
    "shapes = np.vstack((shapes_normal,shapes_pneumonia))\n",
    "debug.extend(debug_normal)\n",
    "debug.extend(debug_pneumonia)\n",
    "\n",
    "Xnormal,Ynormal,shapes_normal,debug_normal = loadData('chest_xray/test/NORMAL',NORMAL)\n",
    "Xpneumonia,Ypneumonia,shapes_pneumonia,debug_pneumonia = loadData('chest_xray/test/PNEUMONIA',PNEUMONIA)\n",
    "Xtest = np.vstack((Xnormal,Xpneumonia))\n",
    "Ytest = Ynormal + Ypneumonia\n",
    "shapes = np.vstack((shapes,shapes_normal,shapes_pneumonia))\n",
    "debug.extend(debug_normal)\n",
    "debug.extend(debug_pneumonia)\n",
    "\n",
    "Xnormal,Ynormal,shapes_normal,debug_normal = loadData('chest_xray/val/NORMAL',NORMAL)\n",
    "Xpneumonia,Ypneumonia,shapes_pneumonia,debug_pneumonia = loadData('chest_xray/val/PNEUMONIA',PNEUMONIA)\n",
    "Xval = np.vstack((Xnormal,Xpneumonia))\n",
    "Yval = Ynormal + Ypneumonia\n",
    "shapes = np.vstack((shapes,shapes_normal,shapes_pneumonia))\n",
    "debug.extend(debug_normal)\n",
    "debug.extend(debug_pneumonia)\n",
    "\n",
    "print(\"Xtrain shape: \",Xtrain.shape)\n",
    "print(\"Ytrain size: \",len(Ytrain))\n",
    "print(\"Xtest shape: \",Xtest.shape)\n",
    "print(\"Ytest size: \",len(Ytest))\n",
    "print(\"Xval shape: \",Xval.shape)\n",
    "print(\"Yval size: \",len(Yval))\n",
    "print(\"input size: \",len(shapes))\n",
    "print(\"another input size:(should be same) \",len(debug))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make new evaluation dataset. (Equal representation from train,test,val and equal normal vs pneumonia representation)\n",
    "Suffle training and testing dataset. \n",
    "Make new testing dataset composed of train and test folder's. (Equal normal vs pneumonia representation)  \n",
    "Resize input data to be compatable for conv2D input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set - normal:  1333  ,pneumonia:  3867\n",
      "train set - normal:  226  ,pneumonia:  382\n",
      "train set - normal:  24  ,pneumonia:  24\n",
      "Xtrain shape:  (5200, 127, 127, 1)\n",
      "Ytrain shape:  5200\n",
      "Xtest shape:  (608, 127, 127, 1)\n",
      "Ytest shape:  608\n",
      "Xval shape:  (48, 127, 127, 1)\n",
      "Yval shape:  48\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def splitEqual(data,labels,size):\n",
    "    Xresult = [np.zeros((len(data[0]),len(data[0])))]\n",
    "    Yresult = []\n",
    "    count_normal = 0\n",
    "    count_pneumonia = 0\n",
    "    i = 0\n",
    "    while count_normal < size or count_pneumonia < size:\n",
    "        if (labels[i] == NORMAL and count_normal < size) or (labels[i] == PNEUMONIA and count_pneumonia < size):\n",
    "            if labels[i] == NORMAL:\n",
    "                count_normal += 1\n",
    "            else:\n",
    "                count_pneumonia += 1\n",
    "            Xresult = np.append(Xresult,[data[i]],axis = 0)\n",
    "            Yresult = Yresult+[labels[i]]\n",
    "            data = np.delete(data,i,axis=0)\n",
    "            del labels[i]\n",
    "        else:\n",
    "            i += 1\n",
    "    Xresult = np.delete(Xresult,0,axis=0)\n",
    "    return (data,labels,Xresult,Yresult)\n",
    "\n",
    "def countLabels(labels):\n",
    "    count_normal = 0\n",
    "    count_pne = 0\n",
    "    for y in labels:\n",
    "        if y == NORMAL:\n",
    "            count_normal +=1\n",
    "        else:\n",
    "            count_pne += 1\n",
    "    return (count_normal,count_pne)\n",
    "\n",
    "Xtrain,Ytrain,Xval1,Yval1 = splitEqual(Xtrain,Ytrain,8)\n",
    "Xtest,Ytest,Xval2,Yval2 = splitEqual(Xtest,Ytest,8)\n",
    "Xval = np.append(Xval,Xval1,axis = 0)\n",
    "Xval = np.append(Xval,Xval2,axis = 0)\n",
    "Yval = Yval + Yval1 + Yval2\n",
    "\n",
    "Xtrain1, Xtrain2, Ytrain1, Ytrain2 = train_test_split(Xtrain,Ytrain, test_size=0.5, shuffle=True, random_state=42)\n",
    "Xtrain = np.vstack((Xtrain1,Xtrain2))\n",
    "Ytrain = Ytrain1+Ytrain2\n",
    "\n",
    "count_normal,count_pne = countLabels(Ytrain)\n",
    "print(\"train set - normal: \",count_normal,\" ,pneumonia: \",count_pne)\n",
    "\n",
    "count_normal,count_pne = countLabels(Ytest)\n",
    "print(\"test set - normal: \",count_normal,\" ,pneumonia: \",count_pne)\n",
    "\n",
    "count_normal,count_pne = countLabels(Yval)\n",
    "print(\"val set - normal: \",count_normal,\" ,pneumonia: \",count_pne)\n",
    "\n",
    "Xtrain = Xtrain.reshape(len(Xtrain),max_size,max_size,1)\n",
    "Xtest = Xtest.reshape(len(Xtest),max_size,max_size,1)\n",
    "Xval = Xval.reshape(len(Xval),max_size,max_size,1)\n",
    "print(\"Xtrain shape: \",Xtrain.shape)\n",
    "print(\"Ytrain shape: \",len(Ytrain))\n",
    "print(\"Xtest shape: \",Xtest.shape)\n",
    "print(\"Ytest shape: \",len(Ytest))\n",
    "print(\"Xval shape: \",Xval.shape)\n",
    "print(\"Yval shape: \",len(Yval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_12 (Conv2D)           (None, 62, 62, 32)        832       \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 62, 62, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_13 (Conv2D)           (None, 62, 62, 8)         2312      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 62, 62, 8)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 31, 31, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_14 (Conv2D)           (None, 31, 31, 16)        1168      \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 16, 16, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_15 (Conv2D)           (None, 16, 16, 32)        4640      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 8, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               262272    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 275,385\n",
      "Trainable params: 275,385\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.metrics import Precision,Recall\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten, InputLayer\n",
    "\n",
    "model = Sequential([\n",
    "    InputLayer((max_size, max_size,1)),\n",
    "    Conv2D(filters=32, kernel_size=5,strides=2, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Conv2D(filters=8,kernel_size=3,strides=1,padding=\"same\", activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling2D(pool_size=2,padding=\"same\"),\n",
    "    Conv2D(filters=16,kernel_size=3,strides=1,padding=\"same\", activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    MaxPooling2D(pool_size=2,padding=\"same\"),\n",
    "    Conv2D(filters=32,kernel_size=3,strides=1,padding=\"same\", activation='relu'),\n",
    "    MaxPooling2D(pool_size=2,padding=\"same\"),\n",
    "    Flatten(),\n",
    "    Dense(128,activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(32,activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer='adam',metrics=[\"accuracy\",Precision(name='precision'),Recall(name='recall')])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model.\n",
    "Use evaluation dataset for validation_data.\n",
    "Use class_weight to balance training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 2, 1: 0.6894233255753814}\n",
      "Epoch 1/25\n",
      "163/163 [==============================] - 36s 223ms/step - loss: 0.4898 - accuracy: 0.7056 - precision: 0.9064 - recall: 0.6736 - val_loss: 0.3473 - val_accuracy: 0.8542 - val_precision: 0.9048 - val_recall: 0.7917\n",
      "Epoch 2/25\n",
      "163/163 [==============================] - 36s 219ms/step - loss: 0.2242 - accuracy: 0.9113 - precision: 0.9707 - recall: 0.9082 - val_loss: 0.2867 - val_accuracy: 0.9375 - val_precision: 0.9200 - val_recall: 0.9583\n",
      "Epoch 3/25\n",
      "163/163 [==============================] - 37s 226ms/step - loss: 0.2040 - accuracy: 0.9212 - precision: 0.9734 - recall: 0.9191 - val_loss: 0.3065 - val_accuracy: 0.8750 - val_precision: 0.8214 - val_recall: 0.9583\n",
      "Epoch 4/25\n",
      "163/163 [==============================] - 36s 221ms/step - loss: 0.1647 - accuracy: 0.9408 - precision: 0.9798 - recall: 0.9397 - val_loss: 0.2195 - val_accuracy: 0.9167 - val_precision: 0.8846 - val_recall: 0.9583\n",
      "Epoch 5/25\n",
      "163/163 [==============================] - 36s 221ms/step - loss: 0.1459 - accuracy: 0.9465 - precision: 0.9828 - recall: 0.9447 - val_loss: 0.2779 - val_accuracy: 0.8542 - val_precision: 0.7742 - val_recall: 1.0000\n",
      "Epoch 6/25\n",
      "163/163 [==============================] - 36s 222ms/step - loss: 0.1382 - accuracy: 0.9492 - precision: 0.9831 - recall: 0.9480 - val_loss: 0.3150 - val_accuracy: 0.7917 - val_precision: 0.7188 - val_recall: 0.9583\n",
      "Epoch 7/25\n",
      "163/163 [==============================] - 38s 232ms/step - loss: 0.1296 - accuracy: 0.9542 - precision: 0.9871 - recall: 0.9509 - val_loss: 0.4768 - val_accuracy: 0.7917 - val_precision: 0.7059 - val_recall: 1.0000\n",
      "Epoch 8/25\n",
      "163/163 [==============================] - 42s 260ms/step - loss: 0.1297 - accuracy: 0.9515 - precision: 0.9860 - recall: 0.9483 - val_loss: 0.2486 - val_accuracy: 0.8750 - val_precision: 0.8000 - val_recall: 1.0000\n",
      "Epoch 9/25\n",
      "163/163 [==============================] - 38s 232ms/step - loss: 0.1064 - accuracy: 0.9602 - precision: 0.9885 - recall: 0.9576 - val_loss: 0.5461 - val_accuracy: 0.7500 - val_precision: 0.6667 - val_recall: 1.0000\n",
      "Epoch 10/25\n",
      "163/163 [==============================] - 38s 235ms/step - loss: 0.0979 - accuracy: 0.9638 - precision: 0.9886 - recall: 0.9625 - val_loss: 0.2741 - val_accuracy: 0.8333 - val_precision: 0.7667 - val_recall: 0.9583\n",
      "Epoch 11/25\n",
      "163/163 [==============================] - 39s 240ms/step - loss: 0.1122 - accuracy: 0.9556 - precision: 0.9882 - recall: 0.9516 - val_loss: 0.2531 - val_accuracy: 0.8542 - val_precision: 0.7742 - val_recall: 1.0000\n",
      "Epoch 12/25\n",
      "163/163 [==============================] - 36s 221ms/step - loss: 0.0922 - accuracy: 0.9667 - precision: 0.9910 - recall: 0.9641 - val_loss: 0.3494 - val_accuracy: 0.8125 - val_precision: 0.7273 - val_recall: 1.0000\n",
      "Epoch 13/25\n",
      "163/163 [==============================] - 36s 220ms/step - loss: 0.0908 - accuracy: 0.9635 - precision: 0.9881 - recall: 0.9625 - val_loss: 0.2912 - val_accuracy: 0.9167 - val_precision: 0.8846 - val_recall: 0.9583\n",
      "Epoch 14/25\n",
      "163/163 [==============================] - 39s 241ms/step - loss: 0.1068 - accuracy: 0.9592 - precision: 0.9893 - recall: 0.9555 - val_loss: 0.2725 - val_accuracy: 0.8750 - val_precision: 0.8000 - val_recall: 1.0000\n",
      "Epoch 15/25\n",
      "163/163 [==============================] - 36s 224ms/step - loss: 0.0875 - accuracy: 0.9652 - precision: 0.9886 - recall: 0.9643 - val_loss: 0.1986 - val_accuracy: 0.9167 - val_precision: 0.8846 - val_recall: 0.9583\n",
      "Epoch 16/25\n",
      "163/163 [==============================] - 37s 227ms/step - loss: 0.0769 - accuracy: 0.9704 - precision: 0.9928 - recall: 0.9672 - val_loss: 0.3020 - val_accuracy: 0.8542 - val_precision: 0.7931 - val_recall: 0.9583\n",
      "Epoch 17/25\n",
      "163/163 [==============================] - 36s 220ms/step - loss: 0.0797 - accuracy: 0.9706 - precision: 0.9908 - recall: 0.9695 - val_loss: 0.2545 - val_accuracy: 0.8750 - val_precision: 0.8000 - val_recall: 1.0000\n",
      "Epoch 18/25\n",
      "163/163 [==============================] - 40s 248ms/step - loss: 0.0657 - accuracy: 0.9746 - precision: 0.9929 - recall: 0.9728 - val_loss: 0.2446 - val_accuracy: 0.8750 - val_precision: 0.8000 - val_recall: 1.0000\n",
      "Epoch 19/25\n",
      "163/163 [==============================] - 38s 233ms/step - loss: 0.0667 - accuracy: 0.9756 - precision: 0.9926 - recall: 0.9744 - val_loss: 0.1314 - val_accuracy: 0.9792 - val_precision: 0.9600 - val_recall: 1.0000\n",
      "Epoch 20/25\n",
      "163/163 [==============================] - 38s 231ms/step - loss: 0.0614 - accuracy: 0.9750 - precision: 0.9926 - recall: 0.9736 - val_loss: 0.2189 - val_accuracy: 0.8750 - val_precision: 0.8000 - val_recall: 1.0000\n",
      "Epoch 21/25\n",
      "163/163 [==============================] - 38s 234ms/step - loss: 0.0651 - accuracy: 0.9740 - precision: 0.9934 - recall: 0.9716 - val_loss: 0.2464 - val_accuracy: 0.8750 - val_precision: 0.8000 - val_recall: 1.0000\n",
      "Epoch 22/25\n",
      "163/163 [==============================] - 37s 224ms/step - loss: 0.0674 - accuracy: 0.9740 - precision: 0.9926 - recall: 0.9723 - val_loss: 0.3219 - val_accuracy: 0.8750 - val_precision: 0.8000 - val_recall: 1.0000\n",
      "Epoch 23/25\n",
      "163/163 [==============================] - 37s 230ms/step - loss: 0.0663 - accuracy: 0.9744 - precision: 0.9916 - recall: 0.9739 - val_loss: 0.2054 - val_accuracy: 0.8958 - val_precision: 0.8276 - val_recall: 1.0000\n",
      "Epoch 24/25\n",
      "163/163 [==============================] - 38s 231ms/step - loss: 0.0532 - accuracy: 0.9794 - precision: 0.9942 - recall: 0.9780 - val_loss: 0.1851 - val_accuracy: 0.9375 - val_precision: 0.8889 - val_recall: 1.0000\n",
      "Epoch 25/25\n",
      "163/163 [==============================] - 37s 230ms/step - loss: 0.0486 - accuracy: 0.9850 - precision: 0.9953 - recall: 0.9845 - val_loss: 0.1026 - val_accuracy: 0.9792 - val_precision: 1.0000 - val_recall: 0.9583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2be3acdddc0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_normal,count_pne = countLabels(Ytrain)\n",
    "weight_for_0 = 2\n",
    "weight_for_1 = (count_normal/count_pne)*2.0 \n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "\n",
    "model.fit(Xtrain, np.array(Ytrain), epochs=25, validation_data=(Xval, np.array(Yval)),class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with the testing dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 1s 29ms/step - loss: 0.4332 - accuracy: 0.8618 - precision: 0.8386 - recall: 0.9660\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[155,  71],\n",
       "       [ 13, 369]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "labels = [0,1]\n",
    "model.evaluate(Xtest, np.array(Ytest))\n",
    "confusion_matrix(Ytest,model.predict_classes(Xtest),labels=labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
